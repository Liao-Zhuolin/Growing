# When Does Self-Supervision Help Graph Convolutional Networks?
## Summary

* 本文总结了三种常用的图像上的自监督方法，并提出可以将自监督方法应用到图的领域，因此提出了适合在图上进行自监督的三种方法。论文没有进行理论证明，只是提出模型并进行相关实验，用实验数据来说明图自监督的作用。本文的突出优点是有条不紊的模型讲述以及细致的实验结果分析。实验很重要，实验结果的分析，找出实验结果中的规律也很很重要。

## Problem Statement
* 现有的神经网络自监督经典方法有哪些；
* 在图方向上如何使用自监督的学习方法；
* 在图方法上加入自监督后有什么效果。
## Methods
* 总结了三种经典的神经网络自监督方法：
  * 预训练-微调：先使用自监督方法进行预训练，使用预训练后的模型参数作为正式训练的初始化参数。该方法存在一个损失函数切换的过程，会存在信息的丢失；
  * 自训练：在训练的过程中将生成的高置信度的结果作为真实的标签，加入训练集中进行训练，该方法存在饱和现象，即当标签率较高的时候，再加入伪标签基本没有效果或者甚至是适得其反，这是因为低标签率的情况下，得到的训练集种子可能较差，通过加入伪标签，引入了较好的训练集种子，而当标签率较高的时候，训练集的种子质量基本全覆盖，所以加入伪标签没有效果；
  * 多任务学习：将自监督方法融入正式训练的过程，用参数将两个损失函数结合起来，论文中提出，多任务学习是三种自监督方法中最好的一种。
* 提出了三种适合于图的自监督方法：
   * 节点特征聚类：利用节点特征之间的相似性来进行聚类，得到的聚类结果就是自监督学习的伪标签。该方法可以将没有边连接且相距较远但特征相似的节点聚类为同一类，适合用在大型的图上，如PubMed；
   * 图属性分割：利用有强连接关系的节点大概率属于同一类这个原则，对图进行分割，分割成为不同的簇，每一个簇里面的所有节点都是属于同一类的，该方法的通用性比较强；
   * 特征补全：将中心的节点特征进行遮掩，利用其邻居特征来对中心的节点的特征进行重构，重构的误差就是自监督的损失函数，该方法既考虑了节点特征，又考虑了边的属性，是回归自监督，适合用在节点邻居较少的数据集上，如CiteSeer。
 * 论文在实验中证明了，图卷积网络加对抗攻击训练和自监督图卷积网络加对抗攻击训练相比，加入自监督后网络的抗干扰能力明显加强。对抗攻击训练即将原始训练集和攻击干扰后的训练集一块进行训练，使用相同的网络和相同的参数。
## Evaluation
* 该论文没有对公式理论的证明和推导，只是对之前的相关方法的一些总结和扩展，所有的结论都是通过实验得出的。该论文对于我的论文也具有借鉴的意义，例如伪标签的数量需要随着标签率的升高而降低。以及我的方法是属于自训练和多任务学习相结合的。我的想法：
  * 作者提出的三种图自监督的方法可以和无监督学习进行结合；
  * 三种自监督的方法都可以加入我的想法之中;
  * 图领域的few-shot learning。
## Conclusion
* 三种自监督方法中效果较好的是多任务学习；
* 三种图特有的自监督方法中图属性分割是最具有普遍性的；
* 对图卷积网络加入自监督，对于提升网络的抗攻击性有帮助。
## Notes
* Self-supervision as an emerging technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generaliz able, and robust representation learning of images. Its introduction to graph convolutional net works (GCNs) operating on graph data is however rarely explored.
* Our codes are available at https://github.com/Ice-iron.
* Graph convolutional networks (GCNs) generalize convolutional neural networks to graph-structured data and exploit the properties of graphs. They have outperformed traditional approaches in numerous graph-based tasks such as node or link classifification , link prediction and graph classifification many of which are semi-supervised learning tasks.
* In a parallel note, self-supervision has raised a surge of interest in the computer vision domain to make use of rich unlabeled data. 
* Semi-supervised graph-based learning works with the crucial assumption that the nodes connected with edges of larger weights are more likely to have the same label.
* To our best knowledge, there has been only one recent workpursuing self-supervision in GCNs.
## References
* Learning from labeled and unlabeled data using graph mincuts.
* Adversarial attack on graph structured data.
* Unsupervised representation learning by predicting image rotations.
* Using self-supervised learning can improve model robustness and uncertainty.
* Multi-stage self-supervised learning for graph convolutional networks.
* Adversarial attacks on neural networks for graph data.
* How powerful are graph neural networks?
